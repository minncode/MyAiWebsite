# CVwithAI Chat App

![CVwithAI Chat App](https://via.placeholder.com/800x400.png?text=CVwithAI+Chat+App) <!-- Add a screenshot for better visuals -->

## ðŸš€ Live Demo

- Frontend: https://cvwithaichat-app.vercel.app/chat  
- Backend: https://myaiwebsite-backend.onrender.com/ask

---

## ðŸ§  Features

- ðŸ¤– AI-powered chat using Hugging Face Inference API  
- âš›ï¸ React frontend with responsive UI  
- ðŸŒ Node.js backend with rate limiting and CORS setup  
- â˜ï¸ Deployed with Vercel (frontend) & Render (backend)  
- ðŸ” Environment variable-based API key management

---

## ðŸ› ï¸ Tech Stack

| Layer       | Tech                         |
|-------------|------------------------------|
| Frontend    | React, React Router DOM      |
| Backend     | Node.js, Express, Axios, CORS|
| Deployment  | Vercel (frontend), Render (backend) |
| API         | Hugging Face `google/gemma-2-9b-it` |
| Others      | Git/GitHub, .env, CSS Modules |

---

## ðŸ› ï¸ Installation and Setup

### ðŸ“¦ Prerequisites

- Node.js (v16 or higher)
- Hugging Face API Key ([Get one here](https://huggingface.co/settings/tokens))

---

### âš™ï¸ Local Setup

#### Backend Setup
cd myaiwebsite-backend
npm install

Create .env:
HF_API_KEY=your_huggingface_api_key

Run server:
node server.js

#### Frontend Setup
cd myaiwebsite
npm install

Create .env:
REACT_APP_BACKEND_URL=http://localhost:5000

Start app:
npm start

---

### ðŸš€ Deployment
Frontend (Vercel)
Domains:
https://cvwithaichat-app.vercel.app
https://cvwithaichat-2wi0hk4do-kim-minsungs-projects.vercel.app
Deployment Commands:
npm run build
vercel --prod
Environment Variables:
REACT_APP_BACKEND_URL=https://myaiwebsite-backend.onrender.com

Backend (Render)
Domain: https://myaiwebsite-backend.onrender.com
Deployment Configuration:
Build Command: npm install
Start Command: node server.js
Environment Variables:
HF_API_KEY=your_huggingface_api_key_here

---

### ðŸ§  Key Learnings and Challenges
## 1. Resolving CORS Issues
Challenge: CORS policy violations due to domain mismatches between Vercel and Render.
Solution:
Implemented dynamic CORS handling in the backend:

app.use(cors({
  origin: (origin, callback) => {
    const allowedOrigins = [
      'https://cvwithaichat-app.vercel.app',
      'https://cvwithaichat-2wi0hk4do-kim-minsungs-projects.vercel.app',
      'http://localhost:3000',
    ];
    if (!origin || allowedOrigins.includes(origin)) {
      callback(null, origin);
    } else {
      callback(new Error('Not allowed by CORS'));
    }
  },
  methods: ['GET', 'POST'],
}));
Adapted to Vercelâ€™s dynamic domain changes by allowing multiple origins.

## 2. Handling Hugging Face API 403 Errors
Challenge: Encountered 403 errors due to lack of access to the google/gemma-2-9b-it model.
Solution:
Requested access to the model on Hugging Face and resolved the issue.
Secured the API key using environment variables for safety.

## 3. Deployment Configuration
Challenge: Port conflicts in Render and environment variable loading issues.
Solution:
Used process.env.PORT for dynamic port binding:
const port = process.env.PORT;
Configured environment variables in both Render and Vercel dashboards.

## 4. Improving AI Response Quality
Challenge: AI responses lacked structure and readability.
Solution:
Applied prompt engineering to enforce structured responses:
const prompt = `
  Respond to users' questions in a clean, structured format.
  - Your answers should be concise and clear.
  - If you use lists, separate each item with a line break and number it.
  - Avoid unnecessary repetition and provide specific advice that reflects the user's context.
  ${userInput}
`;
Enhanced readability by post-processing responses in the frontend using the formatResponse function.

---

### ðŸ”® Future Improvements
Model Enhancement: Transition to a more advanced language model (e.g., OpenAI GPT-3.5) for better performance.
User Experience: Add chat history storage and dark mode support.
Performance Optimization: Implement response caching on the backend.
Multilingual Support: Add Vietnamese and Japanese language support for broader accessibility.
